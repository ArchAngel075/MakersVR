Instruction for using the testing framework to test calibration using single-camera pose estimation and tracking using multi-camera triangulation.

To try Calibration out, select "Start Calibration".
The calibration has several phases and all you have to do is select the focus of the calibration while the calibration marker is randomly moved around in a way that should be optimal for the respective phase.

In the first phase, intrinsic calibration, choose the camera to focus on with the numbers 1-3.
The marker will be held at varying positions close to the camera, and once a certain amount of positions on the screen are covered, it will start a calibration round.
During a calibration round, you can switch to other cameras, as it will not add any marker poses to the selection while a calibration round is ongoing for that camera.
After a calibration round finishes, bad markers are removed and you can add even more markers, until the next calibration round starts.
You can see the calibration results compared to the ground truth after each calibration round for that camera.
Once you're happy with all cameras, press 'B' and wait for the current calibration rounds to finish (they can take super long once you go to 50 markers!) - this will lock the main thread currently, but don't worry. After this, the intrinsic calibration results are saved to calib.json.
Or press 'N' to discard and go to next phase, reusing existing calibration (or none if none existed).

In the second phase, extrinsic calibration, the calibration marker will try to face two cameras at once, so that their relative transform can be estimated.
Choose the camera relation to focus on using the numbers 1-3 again.
Watch each relation get more accurate - this however is calculated using ground truth.
In reality, the only metric to use is weight, anything around 1 is fine.

Save results using 'B' and advance to the third phase, where the camera origin is estimated.
Just wait until all the origin transforms have a decent weight of around 1 or less.

After pressing 'B' again, both relations and origin transforms are combined, all paths are weighted according to weakest link and transforms combined.
This works really well in practice to give each camera the most accurate transform possible while prioritising camera relations over the absolute origin.
This is very important as not each camera is close to the origin, and just relying on the noisy origin pose would result in terrible camera relations, which are more important than the absolute camera position.
If the origin would not be in the cameras view at all, the relative transforms of that camera is used to propagate the origin to that camera.

Now, the extrinsic calibration results are also saved to calib.json (except if you pressed 'N').



After calibration, you can press 'N' twice or restart and use 'Start Tracking' to test several 3D-markers out, which will be detected in a triangulated point cloud. This requires proper calibration, if the markers aren't detected, load the backup calibration or recalibrate properly.